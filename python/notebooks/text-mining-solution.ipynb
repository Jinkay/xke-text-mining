{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XKE Text Mining - Solutions\n",
    "----\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook aims at introducing the user to the processing and analysis of text data in Spark with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x107548910>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if you have a running SparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load data\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the corpus of texts stored in `data/articles_blog/` into a DataFrame\n",
    "\n",
    "> Hint 1: Texts are stored in directories associated with their year and month of release. To load every texts in one DataFrame, you can just use `*` instead of a directory name: \n",
    "\n",
    "`/articles_blog/*/*/*.txt`\n",
    "\n",
    "> Hint 2: Each file has the following structure: Three information separated by `|`. The resulting DataFrame then must have three columns:\n",
    "- title: String\n",
    "- category: String\n",
    "- content: String\n",
    "\n",
    "> Hint 3: One way to proceed is to use the [read](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader) function with the `com.databricks.spark.csv` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_corpus = (sqlContext\n",
    "      .read\n",
    "      .format('com.databricks.spark.csv')\n",
    "      .options(header='false', delimiter='|')\n",
    "      .load('/Users/Yoann/Documents/Xebia/tests/text-mining/articles_blog/*/*/*.txt')\n",
    "     )\n",
    "df_corpus = (df_corpus\n",
    "             .withColumnRenamed('C0', 'title')\n",
    "             .withColumnRenamed('C1', 'category')\n",
    "             .withColumnRenamed('C2', 'content')\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_corpus.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this test to check if you got the right DataFrame size\n",
    "assert(df_corpus.count() == 335)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the stopwords stored in `data/stopwords_french.txt` in an Array[String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = sc.textFile(\"/Users/Yoann/Documents/Xebia/tests/text-mining/stopwords_french.txt\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this test to check if you got the right array size\n",
    "assert(len(stopwords) == 237)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Tokenizer\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is available, it is time to pre-process it before we can use it in algorithms. The first thing to do is to tokenize each text to get an array of tokens (words) that will be used afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a function extract_tokens which transforms a String into an array of tokens. The transformation can perform the following actions:\n",
    "\n",
    "- Split on spaces (mandatory)\n",
    "- Remove punctuation and numbers (can be done with `punctuation.sub(' ', sentence)`) \n",
    "- Convert to lowercase\n",
    "- Remove every stopwords\n",
    "- Keep only words with length strictly higher than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "punctuation = re.compile(r'[-.?!,\":;()|0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_tokens(sentence):\n",
    "    \"\"\"Tokenization of a given sentence.\n",
    "       Drop stopwords, punctuations, numbers.\n",
    "       Change the sentence to lowercase.\"\"\"\n",
    "    \n",
    "    if(sentence) :\n",
    "        tokens = punctuation.sub(' ', sentence.replace(\"'\", \" \")).lower().split()\n",
    "        tokens_filtered = [word for word in tokens if (word not in stopwords) and (len(word) > 2)]\n",
    "        return tokens_filtered\n",
    "    else :\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function\n",
    "extract_tokens(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a Spark UDF (User Defined Function) which uses the previous tokens function\n",
    "\n",
    "> Hint: Use the [udf](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf) function with your previous function in a lambda function\n",
    "\n",
    "##### Add a new column named `tokens` to the df_corpus DataFrame containing the result of the tokenizer UDF used on the `content` column\n",
    "\n",
    "> Hint 1: Use the [withColumn](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn) method of the df_corpus DataFrame\n",
    "\n",
    "> Hint 2: To apply the UDF on a DataFrame column, just do the following: `yourUDF(yourDataFrame.col_name)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create UDF\n",
    "extract_tokens_udf = udf(lambda sentence: extract_tokens(sentence), ArrayType(StringType()))\n",
    "\n",
    "# Add tokens column to DataFrame\n",
    "df_tokens = df_corpus.withColumn('tokens', extract_tokens_udf(df_corpus.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('title', 'string'),\n",
       " ('category', 'string'),\n",
       " ('content', 'string'),\n",
       " ('tokens', 'array<string>')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if your DataFrame has the right column names and types\n",
    "df_tokens.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+\n",
      "|               title|     category|             content|              tokens|\n",
      "+--------------------+-------------+--------------------+--------------------+\n",
      "|2014-01-08-crafts...|        Craft|  Pour coder tous...|[coder, jours, pl...|\n",
      "|2014-01-10-androi...|AndroidMobile|  En tant que dév...|[tant, développeu...|\n",
      "+--------------------+-------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tokens.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are the 10 most used words in the corpus ?\n",
    "\n",
    "> Hint 1: Use the [explode](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.explode) function to have a DataFrame with one word per line\n",
    "\n",
    "> Hint 2: You can perform the following operations\n",
    "- Use the `select` function and use the `explode` function on the `tokens` column, name it \"word\"\n",
    "- Group By the \"word\" column\n",
    "- Use the `count()` function to count the number of occurrences of each word\n",
    "- Order By the count result, [descending](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.desc), and show the 10 first resulting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|       plus| 1576|\n",
      "|      cette|  909|\n",
      "|      c’est|  804|\n",
      "|       d’un|  787|\n",
      "|       code|  625|\n",
      "|      faire|  617|\n",
      "|     projet|  584|\n",
      "|      d’une|  583|\n",
      "|       bien|  576|\n",
      "|     permet|  559|\n",
      "|    données|  527|\n",
      "|      xebia|  458|\n",
      "|    exemple|  418|\n",
      "|      tests|  407|\n",
      "|       test|  406|\n",
      "|        the|  395|\n",
      "|      n’est|  369|\n",
      "|       faut|  360|\n",
      "|       http|  357|\n",
      "|application|  355|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, desc\n",
    "\n",
    "df_words = df_tokens.select(explode(df_tokens.tokens).alias('word'))\n",
    "df_words.groupBy('word').count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You should observe that even with a descent tokenizing, the most used words are still not very usefull to characterize the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Bonus) Using the NGram Transformer, find the most used sequences of 2 consecutive words in the corpus\n",
    "\n",
    "> Hint: Use the [ngram](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.NGram) transformer with n=2 and inputCol=\"tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "bigram = NGram(n=2, inputCol='tokens', outputCol='bigrams')\n",
    "df_bigram = bigram.transform(df_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|         cet article|  149|\n",
      "|          mise place|   92|\n",
      "|        mettre place|   89|\n",
      "|    machine learning|   88|\n",
      "|             col col|   78|\n",
      "|software craftsma...|   78|\n",
      "|          chez xebia|   67|\n",
      "|        scikit learn|   65|\n",
      "|          c’est dire|   65|\n",
      "|           plus plus|   65|\n",
      "|        public class|   61|\n",
      "|         d’un projet|   60|\n",
      "|         public void|   60|\n",
      "|       product owner|   56|\n",
      "|     tests unitaires|   56|\n",
      "|           plus loin|   55|\n",
      "|   d’une application|   54|\n",
      "| continuous delivery|   52|\n",
      "|  cette présentation|   51|\n",
      "|    http //localhost|   47|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_words = df_bigram.select(explode(df_bigram.bigrams).alias('word'))\n",
    "df_words.groupBy('word').count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You should see some combinations with interpretable meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Word2Vec\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tokens, we can use them in some algorithms to extract useful features from them. One algorithm that we can use is [Word2Vec](https://spark.apache.org/docs/latest/ml-features.html#word2vec), which has an implementation in Sparl ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instanciate a new Word2Vec object with the following settings\n",
    "- inputCol: \"tokens\"\n",
    "- outputCol: \"w2c_features\"\n",
    "- vectorSize: 50\n",
    "- minCount: 10\n",
    "- maxIter: 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(vectorSize=50, minCount=10, maxIter=50, inputCol='tokens', outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train a model on the `df_tokens` DataFrame using the `fit` method of your word2Vec object\n",
    "\n",
    "> This might take a few minutes to run depending on the parameters you chose\n",
    "\n",
    "> You can try different parameters from the ones proposed, but avoid providing values that are too high when you don't work on a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2Vec_model = word2Vec.fit(df_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have on our hands a trained Word2Vec model that we can use and query.\n",
    "\n",
    "##### Check how the learning phase went by finding synonyms of a few words of your choice\n",
    "\n",
    "> Hint 1: Use the `findSynonyms(word, num_synonyms)` method called on the learned model\n",
    "\n",
    "> Hint 2: The result of that function is a DataFrame. Use the show method on it to print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|       word|        similarity|\n",
      "+-----------+------------------+\n",
      "|    keynote| 1.556393936797057|\n",
      "|conférences|1.4617819337912439|\n",
      "|d’ouverture|1.3629812805060728|\n",
      "|    édition|1.3451473302159676|\n",
      "|      année|1.3071189316752012|\n",
      "|    journée| 1.305614050319781|\n",
      "|    xebicon|1.2829967047689437|\n",
      "|      talks|1.2686259718399966|\n",
      "|  programme|1.2677530964254287|\n",
      "|  retrouvez| 1.264922965047682|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2Vec_model.findSynonyms(\"conférence\", 10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec seems to do a good job finding synonyms. Now let's look if we can find some associations like `king - man + woman -> queen`.\n",
    "\n",
    "##### Write a function find_nearest_word(model, vectors_df, king_word, man_word, woman_word) which finds the word in the corpus which is nearest to the word of coordinates `king_word - man_word + woman_word`\n",
    "\n",
    "> Hint 1: `model` is your Word2Vec model\n",
    "\n",
    "> Hint 2: `vectors_df` is the DataFrame that comes from the getVectors() method of a Word2Vec model. It has two columns: \n",
    "- word: a word in the corpus\n",
    "- vector: it's associated coordinates\n",
    "\n",
    "> Hint 3: The function may run the following actions\n",
    "- Find the `vector` associated to the `king_word` in vectors_df\n",
    "- Do the same for `man_word` and `woman_word`\n",
    "- Use the findSynonyms function of the model with the vector `king_word - man_word + woman_word` as argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2Vec_vectors_df = word2Vec_model.getVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|    word|              vector|\n",
      "+--------+--------------------+\n",
      "|     dns|[0.09927792102098...|\n",
      "| speaker|[-0.1568965017795...|\n",
      "|  assert|[-0.3278719782829...|\n",
      "|    mise|[0.48140543699264...|\n",
      "|  plugin|[0.35941249132156...|\n",
      "| retenir|[0.14124287664890...|\n",
      "|    afin|[0.14102584123611...|\n",
      "|   alpha|[-0.2422264367341...|\n",
      "|demandes|[-0.5457851290702...|\n",
      "| cyrille|[0.03088080696761...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2Vec_vectors_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_nearest_word(model=None, vectors_df=None, king_word=\"\", man_word=\"\", woman_word=\"\"):\n",
    "    vect_king_word = vectors_df.filter(vectors_df.word == king_word).first().vector\n",
    "    vect_man_word = vectors_df.filter(vectors_df.word == man_word).first().vector\n",
    "    vect_woman_word = vectors_df.filter(vectors_df.word == woman_word).first().vector\n",
    "    return model.findSynonyms(vect_king_word - vect_man_word + vect_woman_word, 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|       word|        similarity|\n",
      "+-----------+------------------+\n",
      "|  collègues|2.3986518909221277|\n",
      "|conférences| 2.275260079800317|\n",
      "|    xebians| 2.215104610800545|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_nearest_word(model=word2Vec_model, vectors_df=word2Vec_vectors_df, \n",
    "                  king_word=\"xebian\", man_word=\"conférence\", woman_word=\"conférences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the `transform()` method of the word2Vec_model, add a new column to the df_tokens DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_features = word2Vec_model.transform(df_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+--------------------+\n",
      "|               title|     category|             content|              tokens|            features|\n",
      "+--------------------+-------------+--------------------+--------------------+--------------------+\n",
      "|2014-01-08-crafts...|        Craft|  Pour coder tous...|[coder, jours, pl...|[0.01064611069030...|\n",
      "|2014-01-10-androi...|AndroidMobile|  En tant que dév...|[tant, développeu...|[-0.0081862772121...|\n",
      "|2014-01-13-separe...|        Craft|  Lors de nos dév...|[lors, développem...|[0.03876683608669...|\n",
      "+--------------------+-------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - KMeans\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we would like to group the articles into clusters in which they share similar topics. \n",
    "\n",
    "For this, we will be using the KMeans algorithm on our corpus. As KMeans needs numerical features to run correctly, we will use the `features` column of our df_features DataFrame as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instanciate a new KMeans object with the following settings\n",
    "- k: an integer of your choice\n",
    "- maxIter: 30\n",
    "- featuresCol: \"features\"\n",
    "- outputCol: \"cluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(k=15, maxIter=30, featuresCol=\"features\", predictionCol=\"cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train a model on the `df_features` DataFrame using the `fit` method of your kmeans object\n",
    "\n",
    "> This might take a few minutes to run depending on the parameters you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans_model = kmeans.fit(df_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the `transform()` method of the kmeans_model, add a new column to the df_features DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_clusters = kmeans_model.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+--------------------+-------+\n",
      "|               title|     category|             content|              tokens|            features|cluster|\n",
      "+--------------------+-------------+--------------------+--------------------+--------------------+-------+\n",
      "|2014-01-08-crafts...|        Craft|  Pour coder tous...|[coder, jours, pl...|[0.01064611069030...|      3|\n",
      "|2014-01-10-androi...|AndroidMobile|  En tant que dév...|[tant, développeu...|[-0.0081862772121...|      3|\n",
      "|2014-01-13-separe...|        Craft|  Lors de nos dév...|[lors, développem...|[0.03876683608669...|      7|\n",
      "|2014-01-14-xebia-...|       Divers|  Xebia aura le p...|[xebia, plaisir, ...|[-0.0384734213196...|      1|\n",
      "|2014-01-15-crafts...|        Craft|  La pratique des...|[pratique, tests,...|[-0.0429301756372...|      3|\n",
      "|2014-01-17-fireba...|    BackFront|     Développer u...|[développer, appl...|[-0.0262986038291...|      3|\n",
      "|2014-01-17-les-di...|        Agile|  Ce petit mémo e...|[petit, mémo, des...|[-0.0523085756978...|      9|\n",
      "|2014-01-20-puppet...|       DevOps|   Premier épisod...|[premier, épisode...|[0.24353924910550...|      7|\n",
      "|2014-01-21-tech-e...| DevOpsEvents|   Une applicatio...|[application, bie...|[0.04900741520310...|     11|\n",
      "|2014-01-22-ceylon...|       Divers|  Ceylon, le nouv...|[ceylon, nouveau,...|[-0.0518821845903...|      1|\n",
      "+--------------------+-------------+--------------------+--------------------+--------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clusters.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to know if the cluster centers represent meaningful topics. To do that, we will use the word2Vec_model to find the synonyms of the cluster centers in our corpus of words.\n",
    "\n",
    "##### Write a function find_synonyms_cluster_center(word2Vec_model, kmeans_model, cluster_id) which finds the synonyms of the cluster center of your choice\n",
    "\n",
    "> Hint 1: `word2Vec_model` is your Word2Vec model, on which you can call the findSynonyms method\n",
    "\n",
    "> Hint 2: `kmeans_model` is your KMeans model, on which you can call the clusterCenters() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_synonyms_cluster_center(word2Vec_model, kmeans_model, cluster_id):\n",
    "    word2Vec_model.findSynonyms(kmeans_model.clusterCenters()[cluster_id], 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|       word|         similarity|\n",
      "+-----------+-------------------+\n",
      "|      mllib| 0.5527822520467677|\n",
      "|   learning| 0.5415354570414984|\n",
      "|algorithmes| 0.5199606945530545|\n",
      "|    données| 0.4988869351536844|\n",
      "|    machine|0.49487814142295816|\n",
      "|      learn|0.48959105512289014|\n",
      "| algorithme| 0.4773796465860811|\n",
      "|     scikit| 0.4722683352209581|\n",
      "|        rdd|0.44398480875690705|\n",
      "|  dataframe| 0.4399253758696312|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_synonyms_cluster_center(word2Vec_model, kmeans_model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def titles_in_cluster(df, cluster_id):\n",
    "    df_cluster = df.filter(df.cluster == cluster_id)\n",
    "    cluster_size = df_cluster.count()\n",
    "    print \"Cluster Size : \" + str(cluster_size)\n",
    "    print \"Cluster Content : \"\n",
    "    for title_row in df_cluster.select('title').collect():\n",
    "        print \"\\t\" + title_row.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Size : 2\n",
      "Cluster Content : \n",
      "\t2014-11-21-techevent-le-3-decembre-venez-decouvrir-watchkit-le-sdk-apple-watch\n",
      "\t2015-02-27-venez-suivre-la-keynote-apple-chez-xebia-2\n"
     ]
    }
   ],
   "source": [
    "titles_in_cluster(df_clusters, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_represented_categories_in_cluster(df, cluster_id):\n",
    "    df_cluster = df.filter(df.cluster == cluster_id)\n",
    "    df_cluster.groupBy('category').count().orderBy(desc('count')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|   Front|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "most_represented_categories_in_cluster(df_clusters, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Bonus) Apply a KMeans to the words to see which words are clustered together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(k=15, maxIter=50, featuresCol=\"vector\", predictionCol=\"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans_model = kmeans.fit(word2Vec_model.getVectors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_clusters_df = kmeans_model.transform(word2Vec_model.getVectors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_clusters_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-------+\n",
      "|            word|              vector|cluster|\n",
      "+----------------+--------------------+-------+\n",
      "|     //localhost|[-0.0727439373731...|      0|\n",
      "|         network|[-0.9946057200431...|      0|\n",
      "|application/json|[0.73389494419097...|      0|\n",
      "|             any|[0.29274126887321...|      0|\n",
      "|             win|[0.02392003312706...|      0|\n",
      "|            name|[-0.0623365081846...|      0|\n",
      "|           count|[-0.9167723059654...|      0|\n",
      "|            demo|[-1.2303223609924...|      0|\n",
      "|         mutable|[-0.0293252132833...|      0|\n",
      "|            your|[-0.0517358668148...|      0|\n",
      "|            rel=|[1.75475168228149...|      0|\n",
      "|          server|[0.74753201007843...|      0|\n",
      "|            path|[-0.5777864456176...|      0|\n",
      "|           using|[0.91474348306655...|      0|\n",
      "|        provider|[-0.9481966495513...|      0|\n",
      "|           email|[-0.9109817743301...|      0|\n",
      "|             yes|[-0.1472924202680...|      0|\n",
      "|            auth|[-0.9163115024566...|      0|\n",
      "|        security|[0.30176281929016...|      0|\n",
      "|             elb|[-0.8262884616851...|      0|\n",
      "+----------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_clusters_df.filter(words_clusters_df.cluster == 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      word|        similarity|\n",
      "+----------+------------------+\n",
      "|      veux|3.5881006251305543|\n",
      "|    listes| 3.502384906595183|\n",
      "|  d’envies|3.4487807050614574|\n",
      "|      tant|3.0072742256851845|\n",
      "|    invité|2.8871016986373803|\n",
      "|constituer|2.7479848202778925|\n",
      "|    privée| 2.056474293140105|\n",
      "|  articles|1.9981329427129113|\n",
      "|   pouvoir|1.9503680088204258|\n",
      "|      lien| 1.913719766184153|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2Vec_model.findSynonyms(kmeans_model.clusterCenters()[14], 10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Bonus) Identify clusters of words that seem unuseful to you and add the corresponding words to the stopwords list. Then tokenize the texts with this new list, transform the resulting DataFrame with the Word2Vec model and run a KMeans. Does it help improving the clusters interpretation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Latent Dirichlet Allocation\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to try another algorithm to infer topics from documents, which is Latent Dirichlet Allocation. First, we need to map each word to an index and filter undesirable words. We can then train a LDA model to find those topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instanciate a new [CountVectorizer](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.CountVectorizer) object with the following settings\n",
    "- minTF: 5.0\n",
    "- minDF: 15.0\n",
    "- featuresCol: \"tokens\"\n",
    "- outputCol: \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", minTF=5.0, minDF=15.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit the model on the `df_tokens` DataFrame, and then add a new column with the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_model = cv.fit(df_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_features = cv_model.transform(df_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+--------------------+\n",
      "|               title|     category|             content|              tokens|            features|\n",
      "+--------------------+-------------+--------------------+--------------------+--------------------+\n",
      "|2014-01-08-crafts...|        Craft|  Pour coder tous...|[coder, jours, pl...|(1360,[0,1,2,4,5,...|\n",
      "|2014-01-10-androi...|AndroidMobile|  En tant que dév...|[tant, développeu...|(1360,[0,1,2,4,5,...|\n",
      "|2014-01-13-separe...|        Craft|  Lors de nos dév...|[lors, développem...|(1360,[0,6,11,12,...|\n",
      "|2014-01-14-xebia-...|       Divers|  Xebia aura le p...|[xebia, plaisir, ...|(1360,[48,99],[12...|\n",
      "|2014-01-15-crafts...|        Craft|  La pratique des...|[pratique, tests,...|(1360,[0,4,9,13,1...|\n",
      "+--------------------+-------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a LD\n",
    "A model, we need to provide a RDD with an index and a Sparse Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_features = (df_features\n",
    "                .filter(df_features.category == \"Data\")\n",
    "                .select(\"features\")\n",
    "                .map(lambda row: row.features)\n",
    "                .zipWithIndex()\n",
    "                .map(lambda (a,b): [b,a])).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instanciate and train new [LDA](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.LDA) object with the following settings\n",
    "- k: 3\n",
    "- maxIterations: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA\n",
    "\n",
    "ldaModel = LDA.train(rdd_features, k=3, maxIterations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can change the parameters of the model, but be careful, the training time can be very long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use the `describeTopics` method of your ldaModel and the `vocabulary` attribute of your cv_model to observe the description in 10 words of each topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 0\n",
      "-------\n",
      "spark\n",
      "données\n",
      "plus\n",
      "algorithmes\n",
      "data\n",
      "machine\n",
      "import\n",
      "learning\n",
      "permet\n",
      "the\n",
      "\n",
      "\n",
      "TOPIC 1\n",
      "-------\n",
      "données\n",
      "plus\n",
      "import\n",
      "the\n",
      "data\n",
      "d’un\n",
      "map\n",
      "docker\n",
      "cluster\n",
      "c’est\n",
      "\n",
      "\n",
      "TOPIC 2\n",
      "-------\n",
      "spark\n",
      "données\n",
      "plus\n",
      "algorithmes\n",
      "data\n",
      "true\n",
      "learning\n",
      "import\n",
      "apache\n",
      "machine\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in range(3):\n",
    "    print \"TOPIC \" + str(topic)\n",
    "    print \"-------\"\n",
    "    for i in ldaModel.describeTopics()[topic][0][:10]:\n",
    "        print cv_model.vocabulary[i]\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
